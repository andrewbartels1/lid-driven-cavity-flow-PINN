{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from lid_driven_cavity_flow_pinn.utils import generate_csv_catalog, read_datafile, get_boundary_samples\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset\n",
    "\n",
    "In this randomly generated dataset I decided to have just a tablular type output with 50,000 samples, 60 features (highly dimensional), and 25 being redundent information for the random and boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        filepath   Re  xsize  ysize\n",
      "1879      ../data/Re300/PUV0.txt  300    151    151\n",
      "1880    ../data/Re300/PUV100.txt  300    151    151\n",
      "1881   ../data/Re300/PUV1000.txt  300    151    151\n",
      "1882  ../data/Re300/PUV10000.txt  300    151    151\n",
      "1883  ../data/Re300/PUV10100.txt  300    151    151 \n",
      " number of files cataloged: 4850\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>Re</th>\n",
       "      <th>xsize</th>\n",
       "      <th>ysize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>../data/Re300/PUV0.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>../data/Re300/PUV100.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>../data/Re300/PUV1000.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>../data/Re300/PUV10000.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>../data/Re300/PUV10100.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>../data/Re2000/PUV9500.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>../data/Re2000/PUV9600.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>../data/Re2000/PUV9700.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>../data/Re2000/PUV9800.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>../data/Re2000/PUV9900.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4850 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filepath    Re  xsize  ysize\n",
       "1879      ../data/Re300/PUV0.txt   300    151    151\n",
       "1880    ../data/Re300/PUV100.txt   300    151    151\n",
       "1881   ../data/Re300/PUV1000.txt   300    151    151\n",
       "1882  ../data/Re300/PUV10000.txt   300    151    151\n",
       "1883  ../data/Re300/PUV10100.txt   300    151    151\n",
       "...                          ...   ...    ...    ...\n",
       "1874  ../data/Re2000/PUV9500.txt  2000    151    151\n",
       "1875  ../data/Re2000/PUV9600.txt  2000    151    151\n",
       "1876  ../data/Re2000/PUV9700.txt  2000    151    151\n",
       "1877  ../data/Re2000/PUV9800.txt  2000    151    151\n",
       "1878  ../data/Re2000/PUV9900.txt  2000    151    151\n",
       "\n",
       "[4850 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the dataset with the number of samples and features with some being redundent and others being useful information\n",
    "# Input, Response = make_classification(n_samples=50000, n_features=35+25, n_informative=35, n_redundant=25, random_state=7406)\n",
    "\n",
    "catalog = generate_csv_catalog()\n",
    "\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data points in a single file 68403\n"
     ]
    }
   ],
   "source": [
    "# pull out a single file and check it out.\n",
    "P, U, V, time, Re = read_datafile(catalog.iloc[1800][\"filepath\"])\n",
    "print(\n",
    "    \"total data points in a single file\",\n",
    "    P.flatten().shape[0] + U.flatten().shape[0] + V.flatten().shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_boundary_samples(P).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe230fa3c0642aa858992e0e7a1081e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# computational boundary\n",
    "x_lower, x_upper, y_lower, y_upper = 0, 1, -1, 0\n",
    "\n",
    "# make a new dataframe with all the data in it\n",
    "P_list, U_list, V_list, time_list, Re_list = [], [], [], [], []\n",
    "P_boundary_list, U_boundary_list, V_boundary_list = [], [], []\n",
    "catalog_dict = catalog.to_dict(\"records\")\n",
    "for file in tqdm(catalog_dict):\n",
    "    P, U, V, time, Re = read_datafile(file[\"filepath\"])\n",
    "\n",
    "    # stash all of them\n",
    "    P_list.append(P.flatten())\n",
    "    U_list.append(U.flatten())\n",
    "    V_list.append(V.flatten())\n",
    "    # select just the boundary points\n",
    "    P_boundary_list.append(get_boundary_samples(P))\n",
    "    U_boundary_list.append(get_boundary_samples(U))\n",
    "    V_boundary_list.append(get_boundary_samples(V))\n",
    "    # don't forget the time and response variable\n",
    "    time_list.append(time.flatten())\n",
    "    Re_list.append(Re)  # response variable\n",
    "\n",
    "P_df = pd.DataFrame(P_list)\n",
    "U_df = pd.DataFrame(U_list)\n",
    "V_df = pd.DataFrame(V_list)\n",
    "P_boundary_df = pd.DataFrame(P_boundary_list)\n",
    "U_boundary_df = pd.DataFrame(U_boundary_list)\n",
    "V_boundary_df = pd.DataFrame(V_boundary_list)\n",
    "# time_df = pd.DataFrame(time_list)\n",
    "Re_df = pd.DataFrame(Re_list)\n",
    "\n",
    "\n",
    "# put together the whole array\n",
    "P_df_al = pd.concat([P_df, pd.DataFrame(time_list), pd.DataFrame(Re_list)], axis=1)\n",
    "U_df_al = pd.concat([U_df, pd.DataFrame(time_list), pd.DataFrame(Re_list)], axis=1)\n",
    "V_df_al = pd.concat([V_df, pd.DataFrame(time_list), pd.DataFrame(Re_list)], axis=1)\n",
    "\n",
    "# put together just the boundary list\n",
    "P_boundary_df_al = pd.concat(\n",
    "    [P_boundary_df, pd.DataFrame(time_list), pd.DataFrame(Re_list)], axis=1\n",
    ")\n",
    "U_boundary_df_al = pd.concat(\n",
    "    [U_boundary_df, pd.DataFrame(time_list), pd.DataFrame(Re_list)], axis=1\n",
    ")\n",
    "V_boundary_df_al = pd.concat(\n",
    "    [V_boundary_df, pd.DataFrame(time_list), pd.DataFrame(Re_list)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some relabeling\n",
    "Re_df = Re_df.rename(columns={0: \"Re\"})\n",
    "Re_df.Re.astype(\"category\")\n",
    "\n",
    "Re_dummy = pd.get_dummies(Re_df.Re)\n",
    "Re_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_df_al_arr = P_df_al.iloc[:, :-1].to_numpy()\n",
    "# norm_pres = V_df_al_arr/np.linalg.norm(V_df_al_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(24, 24))\n",
    "im = ax.imshow(P_df_al.iloc[:, :-1].to_numpy(), cmap=\"jet\", interpolation=\"nearest\")\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(im, orientation=\"vertical\", cax=cax)\n",
    "plt.ylabel(\"time + Re (300 - 2,000)\")\n",
    "plt.xlabel(\"flattened P velocity\")\n",
    "plt.savefig(\n",
    "    \"../images/P_flattened_y_time_Re_colorbar.png\", bbox_inches=\"tight\", dpi=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(24, 24))\n",
    "im = ax.imshow(U_df_al.iloc[:, :-1].to_numpy(), cmap=\"jet\", interpolation=\"nearest\")\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(im, orientation=\"vertical\", cax=cax)\n",
    "plt.ylabel(\"time + Re (300 - 2,000)\")\n",
    "plt.savefig(\n",
    "    \"../images/U_flattened_y_time_Re_colorbar.png\", bbox_inches=\"tight\", dpi=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(24, 24))\n",
    "im = ax.imshow(V_df_al.iloc[:, :-1].to_numpy(), cmap=\"jet\", interpolation=\"nearest\")\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(im, orientation=\"vertical\", cax=cax)\n",
    "plt.ylabel(\"time + Re (300 - 2,000)\")\n",
    "plt.xlabel(\"flattened V velocity\")\n",
    "plt.savefig(\n",
    "    \"../images/V_flattened_y_time_Re_colorbar.png\", bbox_inches=\"tight\", dpi=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(P_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above it's worth noting the dataset is binary! (either 0 or 1)\n",
    "\n",
    "Now let's split this into a test and train split, and do stratified cross-validation on the training dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "Alrighty, now that the test and train data is split, let's train some models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all, shape (4850, 45603)\n",
      "(2425, 136809) (2425, 70206) (2425, 183) (2425, 18)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Setup the data, train first half, test second half\n",
    "P_bound_data_train = np.split(P_boundary_df_al, 2, axis=0)[0]\n",
    "P_bound_data_test = np.split(P_boundary_df_al, 2, axis=0)[1]\n",
    "U_bound_data_train = np.split(U_boundary_df_al, 2, axis=0)[0]\n",
    "U_bound_data_test = np.split(U_boundary_df_al, 2, axis=0)[1]\n",
    "V_bound_data_train = np.split(V_boundary_df_al, 2, axis=0)[0]\n",
    "V_bound_data_test = np.split(V_boundary_df_al, 2, axis=0)[1]\n",
    "\n",
    "# all\n",
    "print(\"all, shape\", P_df_al.shape)\n",
    "\n",
    "Input_all_train_p = np.split(P_df_al, 2, axis=0)[0]\n",
    "Input_all_test_p = np.split(P_df_al, 2, axis=0)[1]\n",
    "Input_all_train_u = np.split(U_df_al, 2, axis=0)[0]\n",
    "Input_all_test_u = np.split(U_df_al, 2, axis=0)[1]\n",
    "Input_all_train_v = np.split(V_df_al, 2, axis=0)[0]\n",
    "Input_all_test_v = np.split(V_df_al, 2, axis=0)[1]\n",
    "\n",
    "Input_train_all = pd.concat(\n",
    "    [Input_all_train_p, Input_all_train_u, Input_all_train_v], axis=1\n",
    ")\n",
    "Input_test_all = pd.concat(\n",
    "    [Input_all_test_p, Input_all_test_u, Input_all_test_v], axis=1\n",
    ")\n",
    "\n",
    "# boundary\n",
    "Input_boundary_train_p = np.split(P_boundary_df_al, 2, axis=0)[0]\n",
    "Input_boundary_test_p = np.split(P_boundary_df_al, 2, axis=0)[1]\n",
    "Input_boundary_train_u = np.split(U_boundary_df_al, 2, axis=0)[0]\n",
    "Input_boundary_test_u = np.split(U_boundary_df_al, 2, axis=0)[1]\n",
    "Input_boundary_train_v = np.split(V_boundary_df_al, 2, axis=0)[0]\n",
    "Input_boundary_test_v = np.split(V_boundary_df_al, 2, axis=0)[1]\n",
    "\n",
    "Input_boundary_train = pd.concat(\n",
    "    [Input_boundary_train_p, Input_boundary_train_u, Input_boundary_train_v], axis=1\n",
    ")\n",
    "Input_boundary_test = pd.concat(\n",
    "    [Input_boundary_test_p, Input_boundary_test_u, Input_boundary_test_v], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# boundary with 10% randomly selected\n",
    "rand_select = np.random.randint(P_bound_data_train.shape[1], size=60)\n",
    "Input_boundary_10_random_train_p = P_bound_data_train[\n",
    "    np.intersect1d(P_bound_data_train.columns, rand_select)\n",
    "]\n",
    "Input_boundary_10_random_test_p = P_bound_data_test[\n",
    "    np.intersect1d(P_bound_data_train.columns, rand_select)\n",
    "]\n",
    "Input_boundary_10_random_train_u = U_bound_data_train[\n",
    "    np.intersect1d(U_bound_data_train.columns, rand_select)\n",
    "]\n",
    "Input_boundary_10_random_test_u = U_bound_data_test[\n",
    "    np.intersect1d(U_bound_data_train.columns, rand_select)\n",
    "]\n",
    "Input_boundary_10_random_train_v = V_bound_data_train[\n",
    "    np.intersect1d(V_bound_data_train.columns, rand_select)\n",
    "]\n",
    "Input_boundary_10_random_test_v = V_bound_data_test[\n",
    "    np.intersect1d(V_bound_data_train.columns, rand_select)\n",
    "]\n",
    "\n",
    "Input_boundary_10_random_train = pd.concat(\n",
    "    [\n",
    "        Input_boundary_10_random_train_p,\n",
    "        Input_boundary_10_random_train_u,\n",
    "        Input_boundary_10_random_train_v,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "Input_boundary_10_random_test = pd.concat(\n",
    "    [\n",
    "        Input_boundary_10_random_test_p,\n",
    "        Input_boundary_10_random_test_u,\n",
    "        Input_boundary_10_random_test_v,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# print(Input_all_train.dtype)\n",
    "Re_dummy_train = np.split(Re_dummy, 2, axis=0)[0]\n",
    "Re_dummy_test = np.split(Re_dummy, 2, axis=0)[1]\n",
    "Re_train = np.split(Re_df, 2, axis=0)[0]\n",
    "Re_test = np.split(Re_df, 2, axis=0)[1]\n",
    "print(\n",
    "    Input_train_all.shape,\n",
    "    Input_boundary_train.shape,\n",
    "    Input_boundary_10_random_train.shape,\n",
    "    Re_dummy_train.shape,\n",
    ")\n",
    "print(type(Input_train_all), type(Re_dummy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        float64\n",
       "1        float64\n",
       "2        float64\n",
       "3        float64\n",
       "4        float64\n",
       "          ...   \n",
       "22797    float64\n",
       "22798    float64\n",
       "22799    float64\n",
       "22800    float64\n",
       "0          int64\n",
       "Length: 68406, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_train_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestModel_all = RandomForestClassifier(\n",
    "    n_jobs=-1, n_estimators=10000, bootstrap=True, random_state=7406\n",
    ").fit(Input_train_all.to_numpy()[:, :-1], Re_dummy_train)\n",
    "print(RandomForestModel_all)\n",
    "RandomForestModel_boundary = RandomForestClassifier(\n",
    "    bootstrap=True, n_estimators=10000, n_jobs=-1, random_state=7406\n",
    ").fit(Input_boundary_train.to_numpy()[:, :-1], Re_dummy_train)\n",
    "RandomForestModel_boundary_10_random_select = RandomForestClassifier(\n",
    "    n_estimators=10000, bootstrap=True, n_jobs=-1, random_state=7406\n",
    ").fit(Input_boundary_10_random_train.to_numpy()[:, :-1], Re_dummy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the model\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# print(confusion_matrix(Re_dummy_test, RandomForestModel_all.predict(Input_test_all.to_numpy()[:, :-1])))\n",
    "# Display accuracy score\n",
    "print(\n",
    "    accuracy_score(\n",
    "        Re_dummy_test, RandomForestModel_all.predict(Input_test_all.to_numpy()[:, :-1])\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "print(\n",
    "    accuracy_score(\n",
    "        Re_dummy_test,\n",
    "        RandomForestModel_boundary.predict(Input_boundary_test.to_numpy()[:, :-1]),\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "print(\n",
    "    accuracy_score(\n",
    "        Re_dummy_test,\n",
    "        RandomForestModel_boundary_10_random_select.predict(\n",
    "            Input_boundary_10_random_test.to_numpy()[:, :-1]\n",
    "        ),\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "# Display F1 score\n",
    "print(\n",
    "    \"f1 score:\",\n",
    "    f1_score(\n",
    "        Re_dummy_test,\n",
    "        RandomForestModel_boundary.predict(\n",
    "            Input_boundary_10_random_test.to_numpy()[:, :-1]\n",
    "        ),\n",
    "        average=\"weighted\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartelsaa/miniconda3/envs/lid_driven_cavity_flow_pin/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/bartelsaa/miniconda3/envs/lid_driven_cavity_flow_pin/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/bartelsaa/miniconda3/envs/lid_driven_cavity_flow_pin/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "GradBoostingClassModel_all = GradientBoostingClassifier(\n",
    "    n_estimators=100, random_state=7406\n",
    ").fit(Input_train_all.to_numpy()[:, :-1], Re_train.to_numpy())\n",
    "\n",
    "GradBoostingClassModel_boundary = GradientBoostingClassifier(\n",
    "    n_estimators=100, random_state=7406\n",
    ").fit(Input_boundary_train.to_numpy()[:, :-1], Re_train.to_numpy())\n",
    "GradBoostingClassModel_boundary_10_random_select = GradientBoostingClassifier(\n",
    "    n_estimators=100, random_state=7406\n",
    ").fit(Input_boundary_10_random_train.to_numpy()[:, :-1], Re_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradBoostingClassModel_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix, f1_score, accuracy_score\n\u001b[1;32m      5\u001b[0m \u001b[39m# print(confusion_matrix(Re_dummy_test, RandomForestModel_all.predict(Input_test_all.to_numpy()[:, :-1])))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Display accuracy score\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(Re_test, GradBoostingClassModel_all\u001b[39m.\u001b[39mpredict(Input_test_all\u001b[39m.\u001b[39mto_numpy()[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(Re_test, GradBoostingClassModel_boundary\u001b[39m.\u001b[39mpredict(Input_boundary_test\u001b[39m.\u001b[39mto_numpy()[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(Re_test, GradBoostingClassModel_boundary_10_random_select\u001b[39m.\u001b[39mpredict(Input_boundary_10_random_test\u001b[39m.\u001b[39mto_numpy()[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradBoostingClassModel_all' is not defined"
     ]
    }
   ],
   "source": [
    "# assess the model\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# print(confusion_matrix(Re_dummy_test, RandomForestModel_all.predict(Input_test_all.to_numpy()[:, :-1])))\n",
    "# Display accuracy score\n",
    "print(\n",
    "    accuracy_score(\n",
    "        Re_test, GradBoostingClassModel_all.predict(Input_test_all.to_numpy()[:, :-1])\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "print(\n",
    "    accuracy_score(\n",
    "        Re_test,\n",
    "        GradBoostingClassModel_boundary.predict(Input_boundary_test.to_numpy()[:, :-1]),\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "print(\n",
    "    accuracy_score(\n",
    "        Re_test,\n",
    "        GradBoostingClassModel_boundary_10_random_select.predict(\n",
    "            Input_boundary_10_random_test.to_numpy()[:, :-1]\n",
    "        ),\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "# Display F1 score\n",
    "pred_Re_gradBoost = GradBoostingClassModel_boundary_10_random_select.predict(\n",
    "    Input_boundary_10_random_test.to_numpy()[:, :-1]\n",
    ")\n",
    "print(pred_Re_gradBoost.shape)\n",
    "print(\n",
    "    \"shape:\",\n",
    "    Input_boundary_10_random_test.to_numpy()[:, :-1].shape,\n",
    "    Input_boundary_10_random_train.to_numpy()[:, :-1].shape,\n",
    ")\n",
    "print(\"f1 score:\", f1_score(Re_test, pred_Re_gradBoost, average=\"weighted\"))\n",
    "# print(\"f1 score:\", f1_score(Re_test, GradBoostingClassModel_boundary.predict(Input_boundary_10_random_test.to_numpy()[:, :-1]), average='weighted'))\n",
    "# print(\"f1 score:\", f1_score(Re_test, GradBoostingClassModel_boundary_10_random_select.predict(Input_boundary_10_random_test.to_numpy()[:, :-1]), average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lid_driven_cavity_flow_pin)",
   "language": "python",
   "name": "lid_driven_cavity_flow_pin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6258e1c9b5da62fbc4b5c55615c1d36e8d6bb87da48e349a90df73fd38453496"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
