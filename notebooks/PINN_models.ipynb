{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from lid_driven_cavity_flow_pinn.utils import generate_csv_catalog, read_datafile, get_boundary_samples\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset\n",
    "\n",
    "In this randomly generated dataset I decided to have just a tablular type output with 50,000 samples, 60 features (highly dimensional), and 25 being redundent information for the random and boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        filepath   Re  xsize  ysize\n",
      "1758  ../data/Re300/PUV10000.txt  300    151    151\n",
      "1759  ../data/Re300/PUV10100.txt  300    151    151\n",
      "1760  ../data/Re300/PUV10200.txt  300    151    151\n",
      "1761  ../data/Re300/PUV10300.txt  300    151    151\n",
      "1762  ../data/Re300/PUV10400.txt  300    151    151 \n",
      " number of files cataloged: 4652\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>Re</th>\n",
       "      <th>xsize</th>\n",
       "      <th>ysize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>../data/Re300/PUV10000.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>../data/Re300/PUV10100.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>../data/Re300/PUV10200.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>../data/Re300/PUV10300.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>../data/Re300/PUV10400.txt</td>\n",
       "      <td>300</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>../data/Re2000/PUV9500.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>../data/Re2000/PUV9600.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>../data/Re2000/PUV9700.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>../data/Re2000/PUV9800.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>../data/Re2000/PUV9900.txt</td>\n",
       "      <td>2000</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4652 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filepath    Re  xsize  ysize\n",
       "1758  ../data/Re300/PUV10000.txt   300    151    151\n",
       "1759  ../data/Re300/PUV10100.txt   300    151    151\n",
       "1760  ../data/Re300/PUV10200.txt   300    151    151\n",
       "1761  ../data/Re300/PUV10300.txt   300    151    151\n",
       "1762  ../data/Re300/PUV10400.txt   300    151    151\n",
       "...                          ...   ...    ...    ...\n",
       "1753  ../data/Re2000/PUV9500.txt  2000    151    151\n",
       "1754  ../data/Re2000/PUV9600.txt  2000    151    151\n",
       "1755  ../data/Re2000/PUV9700.txt  2000    151    151\n",
       "1756  ../data/Re2000/PUV9800.txt  2000    151    151\n",
       "1757  ../data/Re2000/PUV9900.txt  2000    151    151\n",
       "\n",
       "[4652 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the dataset with the number of samples and features with some being redundent and others being useful information\n",
    "# Input, Response = make_classification(n_samples=50000, n_features=35+25, n_informative=35, n_redundant=25, random_state=7406)\n",
    "\n",
    "catalog = generate_csv_catalog()\n",
    "\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data points in a single file 68403\n"
     ]
    }
   ],
   "source": [
    "# pull out a single file and check it out.\n",
    "P, U, V, time, Re = read_datafile(catalog.iloc[1800][\"filepath\"])\n",
    "print(\n",
    "    \"total data points in a single file\",\n",
    "    P.flatten().shape[0] + U.flatten().shape[0] + V.flatten().shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_boundary_samples(P).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2ef33f06c84ffcb3110035a96c2a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4652 4652 4652\n"
     ]
    }
   ],
   "source": [
    "# computational boundary\n",
    "x_lower, x_upper, y_lower, y_upper = 0, 1, -1, 0\n",
    "\n",
    "# make a new dataframe with all the data in it\n",
    "P_list, U_list, V_list, time_list, Re_list = [], [], [], [], []\n",
    "P_boundary_list, U_boundary_list, V_boundary_list = [], [], []\n",
    "catalog_dict = catalog.to_dict(\"records\")\n",
    "for file in tqdm(catalog_dict):\n",
    "    P, U, V, time, Re = read_datafile(file[\"filepath\"])\n",
    "\n",
    "    # stash all of them\n",
    "    P_list.append(P.flatten())\n",
    "    U_list.append(U.flatten())\n",
    "    V_list.append(V.flatten())\n",
    "    # select just the boundary points\n",
    "    P_boundary_list.append(get_boundary_samples(P))\n",
    "    U_boundary_list.append(get_boundary_samples(U))\n",
    "    V_boundary_list.append(get_boundary_samples(V))\n",
    "    # don't forget the time and response variable\n",
    "    time_list.append(time.flatten()[:600])\n",
    "    Re_list.append(Re)  # response variable\n",
    "\n",
    "print(len(time_list), len(P_boundary_list), len(Re_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 300,  400,  500,  600,  700,  800,  900, 1000, 1100, 1200, 1300,\n",
       "       1400, 1500, 1600, 1700, 1800, 1900, 2000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Re_list)  # labels (the Re number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/donny-chan/pinn-torch/blob/5edcd6834a8fddc91db2e9adba958b0b403fd31f/model.py\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn, autograd, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def calc_grad(y, x) -> Tensor:\n",
    "    grad = autograd.grad(\n",
    "        outputs=y,\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones_like(y),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return grad\n",
    "\n",
    "\n",
    "class Pinn(nn.Module):\n",
    "    \"\"\"\n",
    "    `forward`: returns a tensor of shape (D, 3), where D is the number of\n",
    "    data points, and the 2nd dim. is the predicted values of p, u, v.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dims: List[int]):\n",
    "        super().__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.ffn_layers = []\n",
    "        input_dim = 3\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.ffn_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.ffn_layers.append(nn.Tanh())\n",
    "            input_dim = hidden_dim\n",
    "        self.ffn_layers.append(\n",
    "            nn.Linear(input_dim, 1)\n",
    "        )  # 2)) # 1 for the Re prediction number\n",
    "        self.ffn = nn.Sequential(*self.ffn_layers)\n",
    "\n",
    "        self.lambda1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.lambda2 = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor,\n",
    "        t: Tensor,\n",
    "        p: Tensor = None,\n",
    "        u: Tensor = None,\n",
    "        v: Tensor = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        All shapes are (b,)\n",
    "\n",
    "        inputs: t, u, v\n",
    "        labels: Re\n",
    "        \"\"\"\n",
    "        inputs = torch.stack([t, u, v], dim=1)\n",
    "        hidden_output = self.ffn(inputs)\n",
    "        psi = hidden_output[:, 0]\n",
    "        p_pred = hidden_output[:, 0]\n",
    "        u_pred = calc_grad(psi, y)\n",
    "        v_pred = -calc_grad(psi, x)\n",
    "\n",
    "        preds = torch.stack([p_pred, u_pred, v_pred], dim=1)\n",
    "        u_t = calc_grad(u_pred, t)\n",
    "        u_x = calc_grad(u_pred, x)\n",
    "        u_y = calc_grad(u_pred, y)\n",
    "        u_xx = calc_grad(u_x, x)\n",
    "        u_yy = calc_grad(u_y, y)\n",
    "\n",
    "        v_t = calc_grad(v_pred, t)\n",
    "        v_x = calc_grad(v_pred, x)\n",
    "        v_y = calc_grad(v_pred, y)\n",
    "        v_xx = calc_grad(v_x, x)\n",
    "        v_yy = calc_grad(v_y, y)\n",
    "\n",
    "        p_x = calc_grad(p_pred, x)\n",
    "        p_y = calc_grad(p_pred, y)\n",
    "\n",
    "        f_u = (\n",
    "            u_t\n",
    "            + self.lambda1 * (u_pred * u_x + v_pred * u_y)\n",
    "            + p_x\n",
    "            - self.lambda2 * (u_xx + u_yy)\n",
    "        )\n",
    "        f_v = (\n",
    "            v_t\n",
    "            + self.lambda1 * (u_pred * v_x + v_pred * v_y)\n",
    "            + p_y\n",
    "            - self.lambda2 * (v_xx + v_yy)\n",
    "        )\n",
    "        loss = self.loss_fn(u, v, u_pred, v_pred, f_u, f_v)\n",
    "        return {\n",
    "            \"preds\": preds,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "    def loss_fn(self, u, v, u_pred, v_pred, f_u_pred, f_v_pred):\n",
    "        \"\"\"\n",
    "        u: (b, 1)\n",
    "        v: (b, 1)\n",
    "        p: (b, 1)\n",
    "        \"\"\"\n",
    "        loss = (\n",
    "            F.mse_loss(u, u_pred, reduction=\"sum\")\n",
    "            + F.mse_loss(v, v_pred, reduction=\"sum\")\n",
    "            + F.mse_loss(f_u_pred, torch.zeros_like(f_u_pred), reduction=\"sum\")\n",
    "            + F.mse_loss(f_v_pred, torch.zeros_like(f_v_pred), reduction=\"sum\")\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input the data and train the PINN to predict Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(range(len(time_list[0 : len(P_boundary_list)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def dump_json(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def make_text_data_fits_it_sits(\n",
    "    P_boundary_list: List,\n",
    "    U_boundary_list: List,\n",
    "    V_boundary_list: List,\n",
    "    time_list: List,\n",
    "    Re_list: List,\n",
    ") -> List[List[float]]:\n",
    "    output_list = []\n",
    "    header = [\"P\", \"U\", \"V\", \"time\", \"Re\"]\n",
    "    output_list.append(header)\n",
    "    Re_list = [[Re] * len(P_boundary_list[0]) for Re in Re_list]\n",
    "    shortened_time_list = time_list[: len(P_boundary_list)]\n",
    "    # this is probably a stupid way to do it but it works for now\n",
    "    for time_step in zip(\n",
    "        P_boundary_list, U_boundary_list, V_boundary_list, shortened_time_list, Re_list\n",
    "    ):\n",
    "        P, U, V, time, Re = time_step\n",
    "        for P_single, U_single, V_single, time_single, Re_single in zip(\n",
    "            P, U, V, time, Re\n",
    "        ):\n",
    "            output_list.append([P_single, U_single, V_single, time_single, Re_single])\n",
    "\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def load_jsonl(path):\n",
    "    return json.load(path)\n",
    "\n",
    "\n",
    "def dump_json(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def write_data_file_to_jsonl(path_to_json: Path, list_to_jsonify: List):\n",
    "    \"\"\"\n",
    "    should mimic the this format\n",
    "    [\"t\", \"u\", \"v\", \"pressure\", \"Re\"]\n",
    "    [2, 0.001, 0.0, 0.001698680166, 0.0, 0.0]\n",
    "    \"\"\"\n",
    "    dump_json(path_to_json, list_to_jsonify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2791201\n"
     ]
    }
   ],
   "source": [
    "data_ready_to_write = make_text_data_fits_it_sits(\n",
    "    P_boundary_list=P_boundary_list,\n",
    "    U_boundary_list=U_boundary_list,\n",
    "    V_boundary_list=V_boundary_list,\n",
    "    time_list=time_list,\n",
    "    Re_list=Re_list,\n",
    ")\n",
    "\n",
    "\n",
    "print(len(data_ready_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_file_to_jsonl(\n",
    "    path_to_json=Path(\"../data/PINN_input_data.json\"),\n",
    "    list_to_jsonify=data_ready_to_write,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PinnDataset(Dataset):\n",
    "    def __init__(self, data: List[List[float]]):\n",
    "        self.data = data\n",
    "        self.examples = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        headers = [\"t\", \"u\", \"v\", \"p\", \"Re\"]\n",
    "        return {key: self.examples[idx, i] for i, key in enumerate(headers)}\n",
    "\n",
    "\n",
    "def get_dataset(data_path: Path) -> Tuple[PinnDataset, PinnDataset]:\n",
    "    with open(str(data_path)) as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    # remove the header\n",
    "    data.pop(0)\n",
    "    # random.shuffle(data) # turn on random shuffling just uncomment\n",
    "    print(len(data))\n",
    "\n",
    "    split_idx = int(len(data) * 0.5)  # train on lower Re, pre\n",
    "    train_data = data\n",
    "    test_data = data[split_idx:]\n",
    "\n",
    "    train_data = PinnDataset(train_data)\n",
    "    test_data = PinnDataset(test_data)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "2791201\n",
      "Number of parameters: 3043\n",
      "2791200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'t': tensor(-0.0058, grad_fn=<SelectBackward0>),\n",
       " 'u': tensor(1.5612e-06, grad_fn=<SelectBackward0>),\n",
       " 'v': tensor(-1.6840e-06, grad_fn=<SelectBackward0>),\n",
       " 'p': tensor(69.7292, grad_fn=<SelectBackward0>),\n",
       " 'Re': tensor(300., grad_fn=<SelectBackward0>)}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "random.seed(0)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# load the data into the dataloader\n",
    "dataset = make_text_data_fits_it_sits(\n",
    "    P_boundary_list, U_boundary_list, V_boundary_list, time_list, Re_list\n",
    ")\n",
    "\n",
    "print(len(dataset))\n",
    "# Model\n",
    "hidden_dims = [20] * 8\n",
    "model = Pinn(hidden_dims=hidden_dims)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "data_path = Path(\"../data/PINN_input_data.json\")\n",
    "# Data\n",
    "train_data, test_data = get_dataset(data_path.as_posix())\n",
    "\n",
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Trainer for convenient training and testing\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Pinn,\n",
    "        output_dir: Path = None,\n",
    "        lr: float = 0.001,\n",
    "        num_epochs: int = 40,\n",
    "        batch_size: int = 128,\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.lr_step = 5  # Unit is epoch\n",
    "        self.lr_gamma = 0.8\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.log_interval = 1\n",
    "        self.samples_per_ep = 250\n",
    "\n",
    "        if output_dir is None:\n",
    "            self.output_dir = Path(\n",
    "                \"result\",\n",
    "                \"pinn-large-tanh\",\n",
    "                f\"bs{batch_size}\"\n",
    "                f\"-lr{lr}\"\n",
    "                f\"-lrstep{self.lr_step}\"\n",
    "                f\"-lrgamma{self.lr_gamma}\"\n",
    "                f\"-epoch{self.num_epochs}\",\n",
    "            )\n",
    "        else:\n",
    "            self.output_dir = output_dir\n",
    "\n",
    "        print(f\"Output dir: {self.output_dir}\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        args = {}\n",
    "        for attr in [\"lr\", \"lr_step\", \"lr_gamma\", \"num_epochs\", \"batch_size\"]:\n",
    "            args[attr] = getattr(self, attr)\n",
    "        dump_json(self.output_dir / \"args.json\", args)\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=1, gamma=self.lr_gamma\n",
    "        )\n",
    "\n",
    "    def get_last_ckpt_dir(self) -> Path:\n",
    "        ckpt_dirs = list(self.output_dir.glob(\"ckpt-*\"))\n",
    "        ckpt_dirs.sort(key=lambda x: int(x.name.split(\"-\")[-1]))\n",
    "        if len(ckpt_dirs) == 0:\n",
    "            return None\n",
    "        return ckpt_dirs[-1]\n",
    "\n",
    "    def train(self, train_data: PinnDataset, do_resume: bool = True):\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "\n",
    "        sampler = RandomSampler(\n",
    "            train_data,\n",
    "            replacement=True,\n",
    "            num_samples=self.samples_per_ep,\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_data, batch_size=self.batch_size, sampler=sampler\n",
    "        )\n",
    "\n",
    "        print(\"====== Training ======\")\n",
    "        print(f'device is \"{device}\"')\n",
    "        print(f\"# epochs: {self.num_epochs}\")\n",
    "        print(f\"# examples: {len(train_data)}\")\n",
    "        print(f\"# samples used per epoch: {self.samples_per_ep}\")\n",
    "        print(f\"batch size: {self.batch_size}\")\n",
    "        print(f\"# steps: {len(train_loader)}\")\n",
    "        self.loss_history = []\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "\n",
    "        # Resume\n",
    "        last_ckpt_dir = self.get_last_ckpt_dir()\n",
    "        if do_resume and last_ckpt_dir is not None:\n",
    "            print(f\"Resuming from {last_ckpt_dir}\")\n",
    "            self.load_ckpt(last_ckpt_dir)\n",
    "            ep = int(last_ckpt_dir.name.split(\"-\")[-1]) + 1\n",
    "        else:\n",
    "            ep = 0\n",
    "\n",
    "        train_start_time = time()\n",
    "        while ep < self.num_epochs:\n",
    "            print(f\"====== Epoch {ep} ======\")\n",
    "            for step, batch in enumerate(train_loader):\n",
    "                inputs = {k: t.to(device) for k, t in batch.items()}\n",
    "\n",
    "                # Forward\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[\"loss\"]\n",
    "                self.loss_history.append(loss.item())\n",
    "\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if step % self.log_interval == 0:\n",
    "                    losses = outputs[\"losses\"]\n",
    "                    print(\n",
    "                        {\n",
    "                            \"step\": step,\n",
    "                            \"loss\": round(loss.item(), 6),\n",
    "                            \"lr\": round(self.optimizer.param_groups[0][\"lr\"], 4),\n",
    "                            \"lambda1\": round(self.model.lambda1.item(), 4),\n",
    "                            \"lambda2\": round(self.model.lambda2.item(), 4),\n",
    "                            \"u_loss\": round(losses[\"u_loss\"].item(), 6),\n",
    "                            \"v_loss\": round(losses[\"v_loss\"].item(), 6),\n",
    "                            \"f_u_loss\": round(losses[\"f_u_loss\"].item(), 6),\n",
    "                            \"f_v_loss\": round(losses[\"f_v_loss\"].item(), 6),\n",
    "                            \"time\": round(time() - train_start_time, 1),\n",
    "                        }\n",
    "                    )\n",
    "            self.lr_scheduler.step()\n",
    "            self.checkpoint(ep)\n",
    "            print(f\"====== Epoch {ep} done ======\")\n",
    "            ep += 1\n",
    "        print(\"====== Training done ======\")\n",
    "\n",
    "    def checkpoint(self, ep: int):\n",
    "        \"\"\"\n",
    "        Dump checkpoint (model, optimizer, lr_scheduler) to \"ckpt-{ep}\" in\n",
    "        the `output_dir`,\n",
    "        and dump `self.loss_history` to \"loss_history.json\" in the\n",
    "        `ckpt_dir`, and clear `self.loss_history`.\n",
    "        \"\"\"\n",
    "        # Evaluate and save\n",
    "        ckpt_dir = self.output_dir / f\"ckpt-{ep}\"\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Checkpointing to {ckpt_dir}\")\n",
    "        torch.save(self.model.state_dict(), ckpt_dir / \"ckpt.pt\")\n",
    "        torch.save(self.optimizer.state_dict(), ckpt_dir / \"optimizer.pt\")\n",
    "        torch.save(self.lr_scheduler.state_dict(), ckpt_dir / \"lr_scheduler.pt\")\n",
    "        dump_json(ckpt_dir / \"loss_history.json\", self.loss_history)\n",
    "        self.loss_history = []\n",
    "\n",
    "    def load_ckpt(self, ckpt_dir: Path):\n",
    "        print(f'Loading checkpoint from \"{ckpt_dir}\"')\n",
    "        self.model.load_state_dict(torch.load(ckpt_dir / \"ckpt.pt\"))\n",
    "        self.optimizer.load_state_dict(torch.load(ckpt_dir / \"optimizer.pt\"))\n",
    "        self.lr_scheduler.load_state_dict(torch.load(ckpt_dir / \"lr_scheduler.pt\"))\n",
    "\n",
    "    def predict(self, test_data: PinnDataset) -> dict:\n",
    "        batch_size = self.batch_size * 32\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "        print(\"====== Testing ======\")\n",
    "        print(f\"# examples: {len(test_data)}\")\n",
    "        print(f\"batch size: {batch_size}\")\n",
    "        print(f\"# steps: {len(test_loader)}\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()  # We need gradient to predict\n",
    "        all_preds = []\n",
    "        all_losses = []\n",
    "        for step, batch in enumerate(test_loader):\n",
    "            batch = {key: t.to(self.device) for key, t in batch.items()}\n",
    "            outputs = self.model(**batch)\n",
    "            all_losses.append(outputs[\"loss\"].item())\n",
    "            all_preds.append(outputs[\"preds\"])\n",
    "        print(\"====== Testing done ======\")\n",
    "        all_preds = torch.cat(all_preds, 0)\n",
    "        loss = sum(all_losses) / len(all_losses)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": all_preds,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir: result/pinn-large-tanh/bs128-lr0.001-lrstep5-lrgamma0.8-epoch40\n",
      "====== Training ======\n",
      "device is \"cuda:0\"\n",
      "# epochs: 40\n",
      "# examples: 2791200\n",
      "# samples used per epoch: 250\n",
      "batch size: 128\n",
      "# steps: 2\n",
      "====== Epoch 0 ======\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Pinn.forward() got an unexpected keyword argument 'Re'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(train_data)\n",
      "Cell \u001b[0;32mIn[111], line 96\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_data, do_resume)\u001b[0m\n\u001b[1;32m     93\u001b[0m inputs \u001b[39m=\u001b[39m {k: t\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     95\u001b[0m \u001b[39m# Forward\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     97\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_history\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/lid_driven_cavity_flow_pin/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Pinn.forward() got an unexpected keyword argument 'Re'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model)\n",
    "trainer.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = trainer.model.lambda1.item()\n",
    "lambda2 = trainer.model.lambda2.item()\n",
    "print(lambda1, lambda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(test_data)\n",
    "preds = outputs[\"preds\"]\n",
    "loss = outputs[\"loss\"]\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(\"loss:\", loss)\n",
    "print(\"preds:\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.array(test_data.data)\n",
    "p = test_arr[:, 3]\n",
    "u = test_arr[:, 4]\n",
    "v = test_arr[:, 5]\n",
    "p_pred = preds[:, 0]\n",
    "u_pred = preds[:, 1]\n",
    "v_pred = preds[:, 2]\n",
    "\n",
    "# Error\n",
    "err_u = np.linalg.norm(u - u_pred, 2) / np.linalg.norm(u, 2)\n",
    "err_v = np.linalg.norm(v - v_pred, 2) / np.linalg.norm(v, 2)\n",
    "err_p = np.linalg.norm(p - p_pred, 2) / np.linalg.norm(p, 2)\n",
    "\n",
    "err_lambda1 = np.abs(lambda1 - 1.0)\n",
    "err_lambda2 = np.abs(lambda2 - 0.01) / 0.01\n",
    "\n",
    "print(f\"Error in velocity: {err_u:.2e}, {err_v:.2e}\")\n",
    "print(f\"Error in pressure: {err_p:.2e}\")\n",
    "print(f\"Error in lambda 1: {err_lambda1:.2f}\")\n",
    "print(f\"Error in lambda 2: {err_lambda2:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lid_driven_cavity_flow_pin)",
   "language": "python",
   "name": "lid_driven_cavity_flow_pin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
